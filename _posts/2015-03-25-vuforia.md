---
layout: post
title:  "Using Argon.js with Vuforia"
date:   2015-03-25 13:35:15
short_description: "This tutorial shows you how to use Argon.js with Vuforia to do image recognition."
source_directory: tutorial3
---

Vuforia Tutorial
----------------

 
Vuforia is the image tracking system from Qualcomm (https://www.qualcomm.com/products/vuforia), which *argon.js* uses to recognize image from a database and register graphics in relation to the image. This tutorial shows how to use an image from a database within *argon.js*. To use this feature effectively, you need to register with Vuforia and create your own database of images. When you register with Vuforia, you will receive a key to your database. This key guarantees that only the users of your applicaitons can access your database. To use Vuforia in Argon3, you will need to insert an encrypted form of your key into your code.  

For the purposes of this tutorial example, however, we can use the standard "stones and chips" images provided by Vuforia without a developer's key.  On top of the stones image, we will place a box  that spins. On top of the chips images, we place an animated 3D graphic monster. (It looks something like a four-legged lizard  walking in place.)

## Main file (index.html)

Here is the html that goes into the body.  There is no "argon-immersive-context" div in this example. The body contains text that appears when the application is set to Page Mode: the text identifies the example for the user and provides a link to the stones and chips image. 

{% highlight html %}
<body style="background-color:rgba(255,255,255,0.7)"> 
 <div>  
<h2>Vuforia image tracking example</h2> 
<p>This example illustrates the use of Vuforia for image tracking. It is designed to work with the standard <a href="https://developer.vuforia.com/sites/default/files/sample-apps/targets/imagetargets_targets.pdf"> "stones and chips" Vuforia image</a>.</P> </div> 
</body>
{% endhighlight %}

## Code file (app.js)
We start with the standard setup. We are using the immersive context (the camera built into the device). We use webgl to render the scene, including the 3D graphic object that we will present. 

{% highlight js %}
 var options = THREE.Bootstrap.createArgonOptions( Argon.immersiveContext );
  options.renderer = { klass: THREE.WebGLRenderer };
  var three = THREE.Bootstrap( options );
{% endhighlight %}


We add light to our scene, because we will have a 3D object that needs to be lighted. These are three.js objects and methods, which you can find documented at http://threejs.org/docs/: 

{% highlight js %}
var light = new THREE.DirectionalLight( 0xffffff, 1 light.position.set( 0, -4, -4 ).normalize();
three.scene.add( light );
var pointLight = new THREE.PointLight( 0xffffff, 1.5, 1000 ) three.camera.add(pointLight);
{% endhighlight %}

 Tell *argon.js* that we require Vuforia for image tracking:
 
 {% highlight js %}
 Argon.immersiveContext.setRequiredCapabilities('Vuforia')
{% endhighlight %}

##The license key 
 
Then we initialize Vuforia with a license key. If the developer is using his or her own database, then the encrypted licenseKey would go here. We put null and default to the Vuforia chips and stones image. 

 {% highlight js %}
  Argon.Vuforia.initialize({ 
    licenseKey: null, 
    startCamera: true, 
  }) 
  .then(function(api) { 
    // load, activate, and use our dataSet 
    api.loadDataSetFromURL('dataset/StonesAndChips.xml').then(function (dataSet) { 
      dataSet.activate() 
      setupStonesContent(dataSet.trackables.stones) 
      setupChipsContent(dataSet.trackables.chips) 
    }).then(api.startObjectTracker) 
      .then(api.hintMaxSimultaneousImageTargets.bind(api, 2)) 
  })
{% endhighlight %}

## Setting up the trackables

This function sets up the content that will appear over the stones images. In this case we use *three.js* methods to construct and texture the box. This function shows the box when the stones image is recognized by Argon and removes the box when the image is lost (i.e. if the user moves the phone so that the camera can no longer see the image). When the box is first recognized, it spins 10 times, then stops. 

{% highlight js %}
  function setupStonesContent( stonesEntity ) { 
    // create an Object3D from the stones entity 
    var stones = three.argon.objectFromEntity(stonesEntity) 
    // create a box 
    var box = new THREE.Mesh(new THREE.BoxGeometry(50, 50, 50), new THREE.MeshNormalMaterial()) 
    box.position.y = 25 
    var boxSpin = 0 
    // add and spin the box when the stones trackable is found 
    stones.addEventListener('argon:found', function() { 
      stones.add(box) 
      boxSpin = 10 
    }) 
    // remove the box when the stones trackable is lost 
    stones.addEventListener('argon:lost', function() { 
      stones.remove(box) 
    }) 
    // animate the box 
    three.on('update', function() { 
      box.rotation.y += boxSpin * three.Time.delta 
      if (boxSpin > 0) boxSpin -= 10 * three.Time.delta 
      else boxSpin = 0 
    }) 
  } 
{% endhighlight %}

This function sets up the content to appear over the chips image. In this case we load a prebuilt model of a lizard. The function shows the lizard when the chips image is recognized by *argon.js* and removes it when the image is lost. 

{% highlight js %}
  function setupChipsContent( chipsEntity ) { 
    // create an Object3D from the chips entity 
    var chips = three.argon.objectFromEntity(chipsEntity) 
    // add the model when the chips trackable is found 
    chips.addEventListener('argon:found', function() { 
      getModel().then(function(model) { 
        chips.add(model) 
      }) 
    }) 
    // remove the model when the chips trackable is lost 
    chips.addEventListener('argon:lost', function() { 
      getModel().then(function(model) { 
        chips.remove(model) 
      }) 
    }) 
  }
{% endhighlight %}
 
This is the function that loads the lizard model, called by the setupChips function above. 

{% highlight js %}
  var modelPromise = getModel() 
  function getModel() { 
    return modelPromise = modelPromise || new Promise(function(resolve, reject) { 
      // load the model 
      var loader = new THREE.JSONLoader() 
      loader.load( 'monster.js', function ( geometry, materials ) { 
        materials[0].morphTargets = true 
        var faceMaterial = new THREE.MeshFaceMaterial( materials ) 
        var morphMesh = new THREE.MorphAnimMesh( geometry, faceMaterial ) 
        morphMesh.duration = 1000 
        morphMesh.time = 0 
        morphMesh.scale.set(0.1,0.1,0.1) 
        morphMesh.position.set(-100,0,0) 
        morphMesh.matrixAutoUpdate = false 
        morphMesh.updateMatrix() 
        // animate the model 
        three.on('update', function() { 
          morphMesh.updateAnimation( 1000 * three.Time.delta ) 
        }) 
        resolve(morphMesh) 
      }) 
    }) 
  }
{% endhighlight %}

## Testing the application

To test the completed application, you need to print out the stones and chips images or bring them up on a computer screen. Then launch the application on your phone and put it at the images. 
